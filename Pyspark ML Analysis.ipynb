{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark ML Heart and Advertisement Data Analysis\n",
    "# Author: Ashutosh Kumar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a Spark session:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark ML Heart Data Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe by calling read() method on SparkSession/spark object:\n",
    "\n",
    "heart = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "             .load(\"/common_folder/heart.csv\")\n",
    "\n",
    "# used option header true to ignore the first row in csv which is header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+---+----------+---+-----------+--------------+---------------+-------------+--------+--------------------+----+-------------+----+\n",
      "|age|sex|pain type| BP|cholestrol|fbs|resting ecg|max heart rate|exercise angina|ST depression|ST slope|flouroscopy coloured|thal|heart disease|_c14|\n",
      "+---+---+---------+---+----------+---+-----------+--------------+---------------+-------------+--------+--------------------+----+-------------+----+\n",
      "| 70|  1|        4|130|       322|  0|          2|           109|              0|          2.4|       2|                   3|   3|            2|null|\n",
      "| 67|  0|        3|115|       564|  0|          2|           160|              0|          1.6|       2|                   0|   7|            1|null|\n",
      "| 57|  1|        2|124|       261|  0|          0|           141|              0|          0.3|       1|                   0|   7|            2|null|\n",
      "| 64|  1|        4|128|       263|  0|          0|           105|              1|          0.2|       2|                   1|   7|            1|null|\n",
      "| 74|  0|        2|120|       269|  0|          2|           121|              1|          0.2|       1|                   1|   3|            1|null|\n",
      "+---+---+---------+---+----------+---+-----------+--------------+---------------+-------------+--------+--------------------+----+-------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify that the data is loaded in the dataframe :\n",
    "\n",
    "heart.show(5)\n",
    "\n",
    "# Pyspark way of checking:\n",
    "#heart.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing basic analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|min(age)|max(age)|\n",
      "+--------+--------+\n",
      "|      29|      77|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the maximum and minimum age?\n",
    "\n",
    "# importing required functions for the same:\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "heart.select(min(\"age\"), max(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 70|\n",
      "| 67|\n",
      "| 57|\n",
      "| 64|\n",
      "| 74|\n",
      "| 65|\n",
      "| 56|\n",
      "| 59|\n",
      "| 60|\n",
      "| 63|\n",
      "| 59|\n",
      "| 53|\n",
      "| 44|\n",
      "| 61|\n",
      "| 57|\n",
      "| 71|\n",
      "| 46|\n",
      "| 53|\n",
      "| 64|\n",
      "| 40|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new DataFrame with only the 'age' column from the original DataFrame. Save it to a variable, say 'heart_age'.:\n",
    "\n",
    "heart_age= heart[[\"age\"]]\n",
    "heart_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|bucket|\n",
      "+---+------+\n",
      "| 70|   4.0|\n",
      "| 67|   3.0|\n",
      "| 57|   2.0|\n",
      "| 64|   3.0|\n",
      "| 74|   4.0|\n",
      "| 65|   3.0|\n",
      "| 56|   2.0|\n",
      "| 59|   2.0|\n",
      "| 60|   3.0|\n",
      "| 63|   3.0|\n",
      "| 59|   2.0|\n",
      "| 53|   2.0|\n",
      "| 44|   1.0|\n",
      "| 61|   3.0|\n",
      "| 57|   2.0|\n",
      "| 71|   4.0|\n",
      "| 46|   1.0|\n",
      "| 53|   2.0|\n",
      "| 64|   3.0|\n",
      "| 40|   1.0|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bucket age groups to four different buckets:\n",
    "\n",
    "# Importing the required method :\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders =  [29, 40, 50, 60, 70, 80]\n",
    "\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"age\").setOutputCol(\"bucket\")\n",
    "bucketer.transform(heart_age).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|bucket|count|count(age)|\n",
      "+------+-----+----------+\n",
      "|   0.0|   12|        12|\n",
      "|   1.0|   67|        67|\n",
      "|   4.0|   10|        10|\n",
      "|   3.0|   74|        74|\n",
      "|   2.0|  107|       107|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the count of observations for each bucket?\n",
    "\n",
    "# Importing sql functions of PySpark:\n",
    "from pyspark.sql.functions import count,expr\n",
    "\n",
    "bucketed = bucketer.transform(heart_age)\n",
    "bucketed.groupBy(\"bucket\").agg(\n",
    "    count(\"age\").alias(\"count\"),\n",
    "    expr(\"count(age)\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import other data to build PySpark based ML model:\n",
    "\n",
    "#Create dataframe by calling read() method on SparkSession/spark object:\n",
    "\n",
    "advertisementDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "             .load(\"/common_folder/Advertising.csv\")\n",
    "\n",
    "# used option header true to ignore the first row in csv which is header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the data is loaded:\n",
    "\n",
    "advertisementDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|Sales|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, to run linear regression, we need to first divide the data into feature and label columns, i.e., X and Y columns.\n",
    "\n",
    "# Prepare data for Machine Learning.\n",
    "# We need a single feature vector in order to train various ML models. We can do this using VectorAssembler.\n",
    "# And we need two columns only — features and label(“Sales”):\n",
    "\n",
    "# The VectorAssembler is a tool you’ll use in nearly every single pipeline you generate. It helps concatenate all your features\n",
    "# into one big vector you can then pass into an estimator. It takes as input a number of columns of Boolean, Double, or Vector.\n",
    "\n",
    "# Importing the required feature:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler  = VectorAssembler().setInputCols([\"TV\", \"Radio\", \"Newspaper\"]).setOutputCol(\"features\")\n",
    "#va.transform(advertisementDF).show()\n",
    "vadvertisementDF = vectorAssembler.transform(advertisementDF)\n",
    "vadvertisementDF = vadvertisementDF.select(['features', 'Sales'])\n",
    "vadvertisementDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: Sales)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "# Once the feature and label columns are ready, we need to fit a linear regression model. \n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='Sales', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(vadvertisementDF)\n",
    "print(lr.explainParams())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.04262382354938965,0.1708116439330275,0.0]\n",
      "Intercept: 3.7812243412809186\n"
     ]
    }
   ],
   "source": [
    "# Checking the model coefficient and intercepts:\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
